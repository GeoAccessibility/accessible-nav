{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3vov4Slgtk_"
      },
      "source": [
        "# Street View Object Detection\n",
        "In this project, we leverage computer vision techniques to detect and predict accessibility features such as wheelchairs, ramps, etc., in street view videos. The goal is to integrate the output of these detections into our SLAM (Simultaneous Localization and Mapping) model to create a comprehensive and dynamic map of the environment. The following procedures are applied to each video frame:\n",
        "\n",
        "* Camera Calibration: This is the first step where we calibrate the camera to ensure accurate measurements and object detection. The calibration process helps us understand the camera's intrinsic and extrinsic parameters, which are crucial for mapping the image coordinates to real-world coordinates.\n",
        "\n",
        "* Object Detection with YOLOv4: We initiate a video stream as input and run each frame through our YOLOv3 object detection model. The model identifies accessibility features and creates an overlay image that contains bounding boxes of the detected objects. These bounding boxes are then overlaid back onto the subsequent frames of our video stream, providing real-time object detection.\n",
        "\n",
        "* Conversion to Geospatial Data: The outputs of the object detection and depth estimation models, which include the class, bounding box, and depth of each detected object, are then converted into geospatial data. This involves mapping the image coordinates of the detected objects to real-world coordinates, taking into account the camera's field of view, orientation, and location.\n",
        "\n",
        "* Updating the SLAM Map: The geospatial data derived from the object detection and depth estimation models is used to update our SLAM map. This map, which is initially created using the SLAM algorithm, is continuously updated with new information about the location and type of accessibility features in the environment. This results in a dynamic and comprehensive map that can be used for navigation and accessibility planning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTQew7XngtlC"
      },
      "source": [
        "## Camera Calibration\n",
        "\n",
        "we first establish a pattern variable that holds the object points in the (x, y, z) coordinate space of the chessboard. Here, x and y represent the horizontal and vertical indices of the street view frames, respectively, while z is consistently set to 0. These object points remain the same for each calibration image, as we anticipate the same street view frame's pattern in each image.\n",
        "\n",
        "Next, we get the coordinates of the corners of the calibration image.\n",
        "\n",
        "Once we've collected all the points from each image, we compute the camera calibration matrix and distortion coefficients using the cv2.calibrateCamera() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntl5QsgGgtlC"
      },
      "outputs": [],
      "source": [
        "# pattern variable\n",
        "pattern = np.zeros((pattern_size[1] * pattern_size[0], 3), np.float32)\n",
        "pattern[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)\n",
        "\n",
        "# coordinate the corners of image points\n",
        "pattern_points = []\n",
        "image_points = []\n",
        "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "found, corners = cv.cornerSubPix(gray, pattern_size, (11, 11), (-1, -1), criteria)\n",
        "if found:\n",
        "    pattern_points.append(pattern)\n",
        "    image_points.append(corners)\n",
        "\n",
        "# compute camera calibration\n",
        "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(pattern_points, image_points, gray.shape[::-1], None, None)\n",
        "\n",
        "# get the corrected image\n",
        "h, w = img.shape[:2]\n",
        "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\n",
        "dst = cv.undistort(img, mtx, dist, None, newcameramtx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zfwFjJhgtlD"
      },
      "source": [
        "## Video Processing\n",
        "\n",
        "In this section, we will employ the YOLOV3 (You Only Look Once) model to identify objects within the street view video, which includes various accessibility features.\n",
        "\n",
        "The video is processed on a frame-by-frame basis. Each detected object is highlighted with a bounding box and labeled with its class name. This processed video, complete with object detection annotations, is then saved as a new video file for further analysis and review.\n",
        "\n",
        "This approach allows us to visually identify and locate accessibility features within the video, aiding in the assessment of street accessibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5ocNGbWgtlD"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python imageai\n",
        "from google.colab.patches import cv2_imshow\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import os\n",
        "import cv2 as cv\n",
        "from imageai.Detection import ObjectDetection\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests as req\n",
        "import os as os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcspqZcrgtlE"
      },
      "outputs": [],
      "source": [
        "# load pretrained Yolo Model from Coco dataset\n",
        "yolo_model = cv.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
        "\n",
        "# load class names from coco dataset\n",
        "class_names = cv.dnn.readNet('coco.names')\n",
        "print(class_names)\n",
        "\n",
        "# get output layers names\n",
        "output_layers_names = yolo_model.getUnconnectedOutLayersNames()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmiZjCEGgtlE"
      },
      "outputs": [],
      "source": [
        "# load street view video\n",
        "cap = cv.VideoCapture('street_view.mp4')\n",
        "\n",
        "# process video frame by frame\n",
        "frame_id = 0\n",
        "while True:\n",
        "    # get the current frame\n",
        "    _, frame = cap.read()\n",
        "    frame_id += 1\n",
        "\n",
        "    # get the height and width of the frame\n",
        "    height, width, _ = frame.shape\n",
        "\n",
        "    # convert the frame into a blob\n",
        "    blob = cv.dnn.blobFromImage(frame, 1/255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
        "\n",
        "    # set the input of the model\n",
        "    yolo_model.setInput(blob)\n",
        "    layer_outputs = yolo_model.forward(output_layers_names)\n",
        "\n",
        "    # get the bounding boxes, confidences and class ids\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    class_ids = []\n",
        "    for output in layer_outputs:\n",
        "        for detection in output:\n",
        "            # get the class probabilities\n",
        "            scores = detection[5:]\n",
        "\n",
        "            # get the class id\n",
        "            class_id = np.argmax(scores)\n",
        "\n",
        "            # get the confidence\n",
        "            confidence = scores[class_id]\n",
        "\n",
        "            # filter out weak predictions\n",
        "            if confidence > 0.5:\n",
        "                # get the bounding box\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "\n",
        "                # get the top left corner\n",
        "                x = int(center_x - w/2)\n",
        "                y = int(center_y - h/2)\n",
        "\n",
        "                # update the bounding box, confidences and class ids\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    # apply non-max suppression\n",
        "    indexes = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "\n",
        "    # draw the bounding boxes and class labels\n",
        "    font = cv.FONT_HERSHEY_PLAIN\n",
        "    colors = np.random.uniform(0, 255, size=(len(boxes), 3))\n",
        "    for i in indexes.flatten():\n",
        "        # get the bounding box\n",
        "        x, y, w, h = boxes[i]\n",
        "\n",
        "        # get the class label\n",
        "        label = str(classes[class_ids[i]])\n",
        "\n",
        "        # get the confidence\n",
        "        confidence = str(round(confidences[i], 2))\n",
        "\n",
        "        # get the color\n",
        "        color = colors[i]\n",
        "\n",
        "        # draw the bounding box\n",
        "        cv.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
        "\n",
        "        # draw the class label\n",
        "        cv.putText(frame, label + ' ' + confidence, (x, y+20), font, 2, (255, 255, 255), 2)\n",
        "\n",
        "    # show the frame\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # press 'q' to exit\n",
        "    if cv.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "# release the video capture object\n",
        "cap.release()\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xto2tNFFgtlE"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}